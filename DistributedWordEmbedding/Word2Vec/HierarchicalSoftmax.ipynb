{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UmNH0hpOvvML"
   },
   "source": [
    "# Huffman树构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhchwtF7v0oH"
   },
   "source": [
    "Huffman树的节点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkL29fO7vnFf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class HuffmanNode:\n",
    "    def __init__(self, is_leaf=False, value=None, fre=0,\n",
    "                 left=None, right=None):\n",
    "        \"\"\"\n",
    "        如果是中间节点的话，value储存的是中间向量\n",
    "        如果是叶子节点的话，value储存的是word的index\n",
    "        \"\"\"\n",
    "        self.is_leaf = is_leaf\n",
    "        self.value = value\n",
    "        self.fre = fre\n",
    "        self.huffmancode = \"\"\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ansfMFUTwkYx"
   },
   "source": [
    "Huffman树类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZo_8ptnwjZV"
   },
   "outputs": [],
   "source": [
    "class Huffman:\n",
    "    def __init__(self, freq_dict, embed_size):\n",
    "        self.root = None\n",
    "        self.freq_dict = freq_dict\n",
    "        self.embed_size = embed_size\n",
    "        self.huffman_code = {}\n",
    "        node_list = [HuffmanNode(is_leaf=True, value=word, fre=fre) for word, fre in freq_dict.items()]\n",
    "        self.build_tree(node_list)\n",
    "        self.generate_huffman_code()\n",
    "    \n",
    "    def build_tree(self, node_list):\n",
    "        while len(node_list) > 1:\n",
    "            # 找出frequencies最小的两个nodes\n",
    "            index1 = 0\n",
    "            index2 = 1\n",
    "            if node_list[index1].fre > node_list[index2].fre:\n",
    "                index1, index2 = index2, index1\n",
    "            for index in range(2, len(node_list)):\n",
    "                if node_list[index].fre < node_list[index2].fre:\n",
    "                    index2 = index\n",
    "                    if node_list[index1].fre > node_list[index2].fre:\n",
    "                        index1, index2 = index2, index1\n",
    "            \n",
    "            parentnode = HuffmanNode(is_leaf=False,\n",
    "                                     value=np.random.normal(0, 0.1, size=(1, self.embed_size)),\n",
    "                                     fre=node_list[index1].fre+node_list[index2].fre,\n",
    "                                     left=node_list[index2], right=node_list[index1])\n",
    "            \n",
    "            if index1 > index2:\n",
    "                node_list.pop(index1)\n",
    "                node_list.pop(index2)\n",
    "            else:\n",
    "                node_list.pop(index2)\n",
    "                node_list.pop(index1)\n",
    "            \n",
    "            node_list.append(parentnode)\n",
    "        self.root=node_list[0]\n",
    "    \n",
    "    def generate_huffman_code(self):\n",
    "        \"\"\"\n",
    "        用DFS构建HuffmanCode\n",
    "        \"\"\"\n",
    "        stack = [self.root]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            while node.left:\n",
    "                code = node.huffmancode\n",
    "                node.left.huffmancode = code + '1'\n",
    "                if node.right:\n",
    "                    node.right.huffmancode = code + '0'\n",
    "                    stack.append(node.right)\n",
    "                node = node.left\n",
    "            if not node.right:\n",
    "                self.huffman_code[node.value] = node.huffmancode\n",
    "            else:\n",
    "                node.right.huffmancode = code + '0'\n",
    "                stack.append(node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2-qqPQp0lBj"
   },
   "outputs": [],
   "source": [
    "text = ['我', '喜欢', '玩', '手机', '游戏', '绝地求生']\n",
    "fre = [15, 9, 4, 8, 2, 1]\n",
    "word_dict = {word: f for word, f in zip(text, fre)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "z-Ja1jXm00jx",
    "outputId": "1f5d4407-50b6-4560-f4a2-58fb7be25594"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'喜欢': '10', '我': '11', '手机': '01', '游戏': '0001', '玩': '001', '绝地求生': '0000'}"
      ]
     },
     "execution_count": 192,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = Huffman(word_dict, 10)\n",
    "h.huffman_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Rm1Yr1L42ww"
   },
   "source": [
    "# Word2Vec函数构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WS8OJxj-411p"
   },
   "outputs": [],
   "source": [
    "class W2V:\n",
    "    def __init__(self, vocab_size, embed_size, learning_rate, model_type,\n",
    "                 HuffmanTree):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_type = model_type\n",
    "        self.embedding_matrix = np.random.normal(0, 0.1, size=(vocab_size, embed_size))\n",
    "        self.HuffmanTree = HuffmanTree\n",
    "    \n",
    "    def train(self, target, context):\n",
    "        if self.model_type == 'CBOW':\n",
    "            self._CBOW(target, context)\n",
    "        elif self.model_type == 'SkipGram':\n",
    "            self._SkipGram(target, context)\n",
    "\n",
    "    def _sigmoid(self, a):\n",
    "        return 1.0 / (1.0 + np.exp(-a))\n",
    "\n",
    "    def _CBOW(self, target, context):\n",
    "        target_huffman = self.HuffmanTree.huffman_code[target]\n",
    "        Xw = np.zeros(shape=(1, self.embed_size))\n",
    "\n",
    "        for u in context:\n",
    "            Xw += self.embedding_matrix[u]\n",
    "\n",
    "        e = self._search_along_huffman(Xw, target_huffman)\n",
    "\n",
    "        for u in context:\n",
    "            self.embedding_matrix[u] += e[0]\n",
    "    \n",
    "    def _SkipGram(self, target, context):\n",
    "        Vw = self.embedding_matrix[target].reshape(1, -1)\n",
    "        e = np.zeros(shape=[1, self.embed_size])\n",
    "        for u in context:\n",
    "            e += self._search_along_huffman(Vw, self.HuffmanTree.huffman_code[u])\n",
    "        self.embedding_matrix[target] += e[0]\n",
    "\n",
    "\n",
    "    def _search_along_huffman(self, Xw, code):\n",
    "        e = np.zeros(shape=[1, self.embed_size])\n",
    "        node = self.HuffmanTree.root\n",
    "        for level in code:\n",
    "            q = self._sigmoid(Xw @ node.value.T)\n",
    "            g = self.learning_rate * (1 - int(level) - q)\n",
    "            e = e + g * node.value\n",
    "            node.value += g * Xw\n",
    "            if level == '1':\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAeOxHXhPhnR"
   },
   "source": [
    "# 训练数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "L_2tvH-aPlDP",
    "outputId": "ad7feae1-20e5-47ee-b7ed-4ff5c2e8f15d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "JooXELDjP6Ku",
    "outputId": "72ae38b7-9ed6-4062-ed17-ff3c2af40cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2238322.gdoc\t   'Modern Family s01e01 Episode Script | SS.gdoc'\n",
      " 2238322.pdf\t   'Modern Family s01e01 Episode Script | SS.pdf'\n",
      "'Colab Notebooks'   ptb.train.txt\n",
      " fractal.mp4\t    WechatIMG7.jpeg\n"
     ]
    }
   ],
   "source": [
    "path = '/content/drive/My Drive'\n",
    "os.chdir(path)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wk_oGbPLQDAd"
   },
   "outputs": [],
   "source": [
    "with open('ptb.train.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "raw_dataset = [line.split() for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9wuhJl4mRDK"
   },
   "source": [
    "#词频统计并去低频词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UivGB6PiQQR-"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter([word for line in raw_dataset for word in line])\n",
    "counter = {word: fre for word, fre in counter.items() if fre > 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyVOo5uzmUlK"
   },
   "source": [
    "#构建词语与index的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yl0y71eWQvwh"
   },
   "outputs": [],
   "source": [
    "idx_to_word = [word for word, _ in counter.items()]\n",
    "word_to_idx = {word: idx for idx, word in enumerate(idx_to_word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jsRWpfcmaoA"
   },
   "source": [
    "#更新数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4enUxovRBWd"
   },
   "outputs": [],
   "source": [
    "datasets = [[word_to_idx[word] for word in line if word in word_to_idx] for line in raw_dataset]\n",
    "counter = {word_to_idx[word]: fre for word, fre in counter.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dipQOXicmfcO"
   },
   "source": [
    "#Context与Target生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e1Qv9RpuH5wu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_center_context(text, max_window_size):\n",
    "    center = []\n",
    "    contexts = []\n",
    "    for line in text:\n",
    "        center += line\n",
    "        for center_i in range(len(line)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size), min(len(line), center_i + window_size + 1)))\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return center, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrzKxbormjtD"
   },
   "source": [
    "#模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svRIF6GNJAWB"
   },
   "outputs": [],
   "source": [
    "center, contexts = get_center_context(datasets, 5)\n",
    "htree = Huffman(counter, 100)\n",
    "# CBOW\n",
    "model = W2V(len(counter), 100, 0.01, \"CBOW\", htree)\n",
    "# SkipGram\n",
    "model2 = W2V(len(counter), 100, 0.01, \"SkipGram\", htree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "F6XqjtaaLLm7",
    "outputId": "fa78b08a-4993-4a7a-95b3-49482b2e80b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/885720 target words training finish. Time: 16.496s\n",
      "200000/885720 target words training finish. Time: 16.430s\n",
      "300000/885720 target words training finish. Time: 16.421s\n",
      "400000/885720 target words training finish. Time: 16.382s\n",
      "500000/885720 target words training finish. Time: 16.291s\n",
      "600000/885720 target words training finish. Time: 17.316s\n",
      "700000/885720 target words training finish. Time: 16.218s\n",
      "800000/885720 target words training finish. Time: 16.281s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "index = 1\n",
    "start = time.time()\n",
    "for target, context in zip(center, contexts):\n",
    "    if index % 100000 == 0:\n",
    "        print(f\"{index}/{len(center)} target words training finish. Time: {time.time()-start:.3f}s\")\n",
    "        start = time.time()\n",
    "    model.train(target, context)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "8g0uA2JeMaNZ",
    "outputId": "288681d2-e788-4766-d891-55fd35130e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/885720 target words training finish. Time: 76.901s\n",
      "200000/885720 target words training finish. Time: 77.514s\n",
      "300000/885720 target words training finish. Time: 77.470s\n",
      "400000/885720 target words training finish. Time: 76.767s\n",
      "500000/885720 target words training finish. Time: 76.799s\n",
      "600000/885720 target words training finish. Time: 76.750s\n",
      "700000/885720 target words training finish. Time: 77.548s\n",
      "800000/885720 target words training finish. Time: 77.546s\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "start = time.time()\n",
    "for target, context in zip(center, contexts):\n",
    "    if index % 100000 == 0:\n",
    "        print(f\"{index}/{len(center)} target words training finish. Time: {time.time()-start:.3f}s\")\n",
    "        start = time.time()\n",
    "    model2.train(target, context)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWYfJ7EucuTQ"
   },
   "source": [
    "#词相似性计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilhivJa1cwlS"
   },
   "outputs": [],
   "source": [
    "def get_similarity(word, k, embed):\n",
    "    idx = word_to_idx[word]\n",
    "    W = embed\n",
    "    x = embed[idx].reshape(1, -1)\n",
    "    cos = (W * x).sum(axis=-1) / np.sqrt((W * W).sum(axis=-1) * (x * x).sum(axis=-1))\n",
    "    cos_indice = [(index, value) for index, value in enumerate(cos)]\n",
    "    cos_indice = sorted(cos_indice, key=lambda x: x[1], reverse=True)\n",
    "    for i in range(1, k+1):\n",
    "        print(f\"Cosine similarity = {cos_indice[i][1]:.3f}: {idx_to_word[cos_indice[i][0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "PyqFlOtgdNs0",
    "outputId": "6790b65e-9e13-46de-db46-0bbcd1ac9156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Hierarchical Softmax:\n",
      "Cosine similarity = 0.715: he\n",
      "Cosine similarity = 0.611: it\n",
      "Cosine similarity = 0.528: ms.\n",
      "Cosine similarity = 0.526: mrs.\n"
     ]
    }
   ],
   "source": [
    "print(\"CBOW Hierarchical Softmax:\")\n",
    "get_similarity('she', 4, model.embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "mOF2xiyqhcmH",
    "outputId": "4eb3e5db-d56b-4ae8-b674-b1ebe93e18ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram Hierarchical Softmax:\n",
      "Cosine similarity = 0.652: ms.\n",
      "Cosine similarity = 0.645: her\n",
      "Cosine similarity = 0.638: mrs.\n",
      "Cosine similarity = 0.624: he\n"
     ]
    }
   ],
   "source": [
    "print(\"SkipGram Hierarchical Softmax:\")\n",
    "get_similarity('she', 4, model2.embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Word2Vec-HierarchicalSoftmax.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
