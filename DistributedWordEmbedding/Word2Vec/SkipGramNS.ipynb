{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7USt-EqQ2DzJ"
   },
   "source": [
    "# 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6SrgKTb2LAd"
   },
   "source": [
    "本实验将使用Pytorch实现word2vec的SkipGram模型，并采用了Negative Sampling的策略，同时使用了来自语料库的Penn Tree Bank的数据集`PTB.train.txt`对模型进行训练，得到了词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpYxKeVd2qaf"
   },
   "source": [
    "# 数据处理与下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CE3V9BTN2uIM"
   },
   "source": [
    "因为本实验是在Google Colab上进行的，所以需要先挂在Google的drive。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "X-dK2l8T22Xv",
    "outputId": "65d415a9-7040-4521-84e1-851955f0856d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      " 2238322.gdoc\t   'Modern Family s01e01 Episode Script | SS.gdoc'\n",
      " 2238322.pdf\t   'Modern Family s01e01 Episode Script | SS.pdf'\n",
      "'Colab Notebooks'   ptb.train.txt\n",
      " fractal.mp4\t    WechatIMG7.jpeg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "path = \"/content/drive/My Drive\"\n",
    "os.chdir(path)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_0esGiG3nOC"
   },
   "source": [
    "由上我们可以看到，此时我们已经切换到了所需训练文件所在的路径了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyC-5FLw30iH"
   },
   "source": [
    "我们可以先读取部分的数据，并观察一下数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6tpXxieE36zb",
    "outputId": "0016f461-e062-4cd8-ce54-039fc47d9ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in the dataset is 42068.\n"
     ]
    }
   ],
   "source": [
    "with open(\"ptb.train.txt\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"The number of lines in the dataset is {len(lines)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5nxHY3Z4ykC"
   },
   "source": [
    "我们可以看到数据集由4万多条文本组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nq6mbfjK5Bza"
   },
   "source": [
    "现在我们对利用空格进行粗分词，并对文本前三行进行次数的统计和前6个词语的显示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "r733vwHT5Api",
    "outputId": "ee655dc4-97c4-45b3-9ec3-7be9fe687599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of word in the 1 line is 24.\n",
      "The first 6 word in the 1 line is ['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett']\n",
      "\n",
      "The number of word in the 2 line is 15.\n",
      "The first 6 word in the 2 line is ['pierre', '<unk>', 'N', 'years', 'old', 'will']\n",
      "\n",
      "The number of word in the 3 line is 11.\n",
      "The first 6 word in the 3 line is ['mr.', '<unk>', 'is', 'chairman', 'of', '<unk>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [line.split() for line in lines]\n",
    "for i in range(3):\n",
    "    print(f\"The number of word in the {i+1} line is {len(datasets[i])}.\")\n",
    "    print(f\"The first 6 word in the {i+1} line is {datasets[i][:6]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyLgjjMO54zT"
   },
   "source": [
    "我们可以看到数据集中数字都被转换为'N'表示，生僻词都转换为'`<unk>`'表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJn2te5F6vZc"
   },
   "source": [
    "由此我们得到了模型训练所需要的数据集`datasets`了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62gDtYaV63a4"
   },
   "source": [
    "# 词语索引建立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_yR0NwG674J"
   },
   "source": [
    "只有文本数据是没有用的，我们需要做的是对数据集进行词频的统计，为了训练的计算简单，把词频低于5的词语丢弃，同时建立词语和索引之间的互相映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MzZ5ZT-6GfO"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter([word for line in datasets for word in line])\n",
    "counter = dict(filter(lambda x: x[1] > 5, counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iZZ6Cow98tUl",
    "outputId": "35182e8a-3e91-4056-8eb5-06008ec11f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of word in the vocabulary is 9582.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of word in the vocabulary is {len(counter)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEsDsO4P9SRF"
   },
   "source": [
    "建立词语和索引之间的互相映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Qh9crbfe9WP0",
    "outputId": "65666697-7c09-4875-cdc4-054fb59f0261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of car is 3786.\n",
      "The word of index 3786 is car.\n"
     ]
    }
   ],
   "source": [
    "idx_to_word = [word for word, _ in counter.items()]\n",
    "word_to_idx = {word: idx for idx, word in enumerate(idx_to_word)}\n",
    "index = word_to_idx['car']\n",
    "print(f\"The index of car is {index}.\")\n",
    "print(f\"The word of index {index} is {idx_to_word[index]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtcIuESOCpi4"
   },
   "source": [
    "将`datasets`里的所有word转换为index："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "re0PGtvNCxl6"
   },
   "outputs": [],
   "source": [
    "datasets = [[word_to_idx[word] for word in line if word in word_to_idx] for line in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mSkOFvREzQo"
   },
   "outputs": [],
   "source": [
    "num_tokens = sum([len(line) for line in datasets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_ep1AIQ97rD"
   },
   "source": [
    "# 二次采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KakTW-vv997F"
   },
   "source": [
    "文本中会有很多的高频词，譬如：`the`等，这些高频词对于词向量的构建来说相较于低频词`join`等的重要性要低，所以我们要根据词频进行二次采样，以一定的概率丢掉部分的高频词，概率计算公式如下：\n",
    "$$\n",
    "P(w_i)=\\max\\left(1-\\sqrt{\\frac{t}{f(w_i)}},0\\right),\n",
    "$$\n",
    "其中$f(w_i)$表示的是词$w_i$的词频，$t$为超参数，这里设为$10^{-4}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jjNmmLlAfxQ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def throw(idx):\n",
    "    return random.uniform(0, 1) < (1 - math.sqrt(1e-4 / counter[idx_to_word[idx]] * num_tokens))\n",
    "\n",
    "subsampling = [[idx for idx in line if not throw(idx)] for line in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pjipTGqEOCE"
   },
   "source": [
    "由此我们得到二次采样后的新数据集，我们可以比较一下新数据集与原数据集的差别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "9YhiC5QbEDHQ",
    "outputId": "66cdbf82-9c9a-4b52-883f-06568f72cf88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in subsampling dataset is 373692.\n",
      "The number of words in original dataset is 885720.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of words in subsampling dataset is {sum([len(line) for line in subsampling])}.\")\n",
    "print(f\"The number of words in original dataset is {sum([len(line) for line in datasets])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xd1_w-HlFPG1"
   },
   "source": [
    "我们也可以看一下高频词`the`的频数的变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "PyKxdN_oFOoc",
    "outputId": "d610af97-d5e9-47b1-92b9-fc829edbafd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before subsampling, the frequency of 'the' is 50770.\n",
      "After subsampling, the frequency of 'the' is 2091.\n"
     ]
    }
   ],
   "source": [
    "before_the_num = sum([line.count(word_to_idx['the']) for line in datasets])\n",
    "after_the_num = sum([line.count(word_to_idx['the']) for line in subsampling])\n",
    "print(f\"Before subsampling, the frequency of 'the' is {before_the_num}.\")\n",
    "print(f\"After subsampling, the frequency of 'the' is {after_the_num}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWehSOUUF16B"
   },
   "source": [
    "我们也可以看一下低频词`car`的频数的变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "edW2UDqQFdBK",
    "outputId": "f0b7ce9a-772a-4591-fe02-e39fdd223795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before subsampling, the frequency of 'car' is 182.\n",
      "After subsampling, the frequency of 'car' is 132.\n"
     ]
    }
   ],
   "source": [
    "before_car_num = sum([line.count(word_to_idx['car']) for line in datasets])\n",
    "after_car_num = sum([line.count(word_to_idx['car']) for line in subsampling])\n",
    "print(f\"Before subsampling, the frequency of 'car' is {before_car_num}.\")\n",
    "print(f\"After subsampling, the frequency of 'car' is {after_car_num}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9a8y9edGEL2"
   },
   "source": [
    "我们可以看到低频词的频数的变化远小于高频词的频数变化，这符合我们的预期，接下来我们将在subsampling数据集进行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMPqMjESGRkt"
   },
   "source": [
    "# 提取center word与context word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjSRnOcjGhyE"
   },
   "source": [
    "对于每一个center word，我们随机在1到`max_window_size`选一个整数作为context窗口的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmQiflSAGx2Q"
   },
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(datasets, max_window_size=4):\n",
    "    centers, contexts = [], []\n",
    "    for line in datasets:\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        for i in range(len(line)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i - window_size), min(len(line), i + window_size + 1)))\n",
    "            indices.remove(i)\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTNr06tPKAP5"
   },
   "source": [
    "函数测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "9wnd_b28IHry",
    "outputId": "ddf8655f-aca9-4a2e-aea7-8fb35d6e23c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center word: 0 -> Context words: [1].\n",
      "Center word: 1 -> Context words: [0, 2, 3].\n",
      "Center word: 2 -> Context words: [1, 3].\n",
      "Center word: 3 -> Context words: [2, 4].\n",
      "Center word: 4 -> Context words: [2, 3, 5, 6].\n",
      "Center word: 5 -> Context words: [3, 4, 6, 7].\n",
      "Center word: 6 -> Context words: [4, 5, 7].\n",
      "Center word: 7 -> Context words: [5, 6].\n",
      "Center word: 8 -> Context words: [9, 10].\n",
      "Center word: 9 -> Context words: [8, 10].\n",
      "Center word: 10 -> Context words: [9].\n"
     ]
    }
   ],
   "source": [
    "toy_data = [list(range(8)), [8, 9, 10]]\n",
    "toy_centers, toy_contexts = get_centers_and_contexts(toy_data, 2)\n",
    "for cen, cont in zip(toy_centers, toy_contexts):\n",
    "    print(f\"Center word: {cen} -> Context words: {cont}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOVUKQO-KCrR"
   },
   "source": [
    "接下来我们要提取数据集的所有center word和context words："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwWQ_hANKJeU"
   },
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampling, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70ctglenKjdF"
   },
   "source": [
    "# 负采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JG8tB3xiKp1M"
   },
   "source": [
    "对于一对的center word和context word，我们要提取$K$个noise word，这里$K$取5，以词频的0.75次方概率从词典中取词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IP2Kj8mkKo3m"
   },
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, K, sampling_weights):\n",
    "    i = 0\n",
    "    all_negatives = []\n",
    "    negatives_candidate = []\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for context in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(context) * K:\n",
    "            if i == len(negatives_candidate):\n",
    "                negatives_candidate = random.choices(population, sampling_weights, k=int(1e5))\n",
    "                i = 0\n",
    "            neg, i = negatives_candidate[i], i+1\n",
    "            if neg not in set(context):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOGDtSnVOJSm"
   },
   "outputs": [],
   "source": [
    "sampling_weights = [counter[word] ** 0.75 for word in word_to_idx]\n",
    "all_negatives = get_negatives(all_contexts, 5, sampling_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tLuRCV3hLFP"
   },
   "source": [
    "# 数据读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHFu7tAThNcJ"
   },
   "source": [
    "在获得了对应的`all_centers`，`all_contexts`和`all_negatives`之后，我们要批量读取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNSx_T1If4-Z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GW0HdZYFmIbA"
   },
   "source": [
    "一个小批量的数据会包含center word以及它的context和negative，这里我们打算把context和negative合并在一起为一个序列，但因为每个center word对应的context，negative pair的长度都是不一样，这时需要给每个序列补长，使他们达到相同的长度，这里直接补0就可以了，但这里要注意的是损失函数计算的时候，这些补充的元素并不参与计算，所以要给每个元素增添上一个mask，如果该word是补码的话，就为0，不是的话，就为1，因为我们把context和negative和在一起了，我们也需要一个label vector，用来标记哪个是正例，用1标记context，其他为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYQRXHkAmG5u"
   },
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    max_len = max([len(context) + len(neg) for _, context, neg in data])\n",
    "    centers = []\n",
    "    contexts_negatives = []\n",
    "    masks = []\n",
    "    labels = []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        contexts_negatives.append(context + negative + [0] * (max_len - cur_len))\n",
    "        centers.append(center)\n",
    "        masks.append([1] * cur_len + [0] * (max_len - cur_len))\n",
    "        labels.append([1] * len(context) + [0] * (max_len - len(context)))\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93finy1BplnA"
   },
   "source": [
    "利用DataLoader批量读取数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "ubqGY6YUpptV",
    "outputId": "d955b217-cc51-44e4-eace-a63e3c3780b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers word shape: torch.Size([512, 1])\n",
      "context_negatives word shape: torch.Size([512, 60])\n",
      "masks word shape: torch.Size([512, 60])\n",
      "labels word shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "data = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True,\n",
    "                                        collate_fn=batchify)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip([\"centers\", \"context_negatives\", \"masks\", \"labels\"], batch):\n",
    "        print(f\"{name} word shape: {data.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KonHt9h4rqvS"
   },
   "source": [
    "# SkipGram模型前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXhyQ6gmtA7C"
   },
   "outputs": [],
   "source": [
    "def skipGram(center, context_negative, embed_v, embed_u):\n",
    "    v = embed_v(center) # [batch_size, embed_size]\n",
    "    u = embed_u(context_negative) # [batch_size, seq_len, embed_size]\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) # [batch_size, 1, seq_len]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8HtyxqErtj2"
   },
   "source": [
    "Loss function："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zpjl-0YfvXcd"
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, labels, mask=None):\n",
    "        pred = pred.float()\n",
    "        labels = labels.float()\n",
    "        mask = mask.float()\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(pred, labels, weight=mask,\n",
    "                                                              reduction='none')\n",
    "        return loss.mean(dim=1)\n",
    "    \n",
    "Loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "PfSnRrd31Ouo",
    "outputId": "82312453-a994-4f85-e55e-14e55c9f84c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2014, 0.8544, 0.3133, 2.1269],\n",
       "        [0.2873, 1.0375, 2.3051, 0.0000]])"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]]).float()\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]]).float()\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]]).float()\n",
    "loss = nn.functional.binary_cross_entropy_with_logits(pred, label, weight=mask, reduction='none')\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "2B0J4zCU1ujQ",
    "outputId": "15f339c6-6a8a-46ca-a033-c06280933466"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.sum(dim=-1)/mask.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rsdzv8KqwoKe"
   },
   "source": [
    "Model Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jc-wC9lQwp1I"
   },
   "outputs": [],
   "source": [
    "embed_size = 125\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_word), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_word), embedding_dim=embed_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1b6BuAuxMJi"
   },
   "source": [
    "Training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCKkkr2hxNwp"
   },
   "outputs": [],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"train on {device}\")\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        Loss_sum = 0.\n",
    "        n = 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skipGram(center, context_negative, net[0], net[1])\n",
    "            loss = (Loss(pred.view(label.shape), label, mask).sum(dim=-1) / mask.sum(dim=-1)).mean() \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            Loss_sum += loss.cpu().item()\n",
    "            n += 1\n",
    "        if (epoch + 1 ) % 10 == 0:\n",
    "            print(f\"epoch {epoch+1}/{num_epochs}, loss {Loss_sum/n:.2f}, time {time.time()-start:.2f}s.\")\n",
    "        loss_list.append(Loss_sum / n)\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "WG98Pt1hFG1t",
    "outputId": "9b539746-75c0-4f8b-ad52-bce0ac2fe40f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch 10/50, loss 3.76, time 5.90s.\n",
      "epoch 20/50, loss 3.37, time 5.87s.\n",
      "epoch 30/50, loss 3.25, time 5.54s.\n",
      "epoch 40/50, loss 3.19, time 5.50s.\n",
      "epoch 50/50, loss 3.16, time 5.80s.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "loss_list = train(net, 0.01, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "c0BpRrDsNsFs",
    "outputId": "49683b59-430c-41d4-bba6-aec8018bf3c1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAFNCAYAAADRi2EuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvAAAAJrk/wN5KKJDqD94IgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_list, marker='s')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Word2Vec Skip-Gram Negative Sampling Model Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4J_XOZIkJ8LN"
   },
   "source": [
    "# 相似词寻找"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16DQqvGwKEXc"
   },
   "source": [
    "我们利用word2vec词向量来探究相似词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00aSkFehKDmV"
   },
   "outputs": [],
   "source": [
    "def get_similar_k(word, k, embed):\n",
    "    W = (embed[1].weight.data + embed[0].weight.data) * 0.5\n",
    "    x = W[word_to_idx[word]]\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=-1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print(f\"cosine similarity={cos[i]:.3f}: {idx_to_word[i]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "fS5mdvgKLAMz",
    "outputId": "6718e958-cfd3-49aa-f4ed-01ac316ff1ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.411: her.\n",
      "cosine similarity=0.404: mother.\n",
      "cosine similarity=0.395: women.\n",
      "cosine similarity=0.388: pregnant.\n"
     ]
    }
   ],
   "source": [
    "get_similar_k('woman', 4, (net[0], net[1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SkipGramNS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
