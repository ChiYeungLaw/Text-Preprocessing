{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGz5Ddhs9yKc"
   },
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FH-xjGlL914M"
   },
   "outputs": [],
   "source": [
    "raw_txt = \"There is also a corpus of instant messaging chat sessions, originally collected by the Naval Postgraduate School for research on automatic detection of Internet predators.\\nThe corpus contains over 10,000 posts, anonymized by replacing usernames with generic names of the form 'UserNNN', and manually edited to remove any other identifying information.\\nThe corpus is organized into 15 files, where each file contains several hundred posts collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a generic adults chatroom).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "7Y3B605n-QMv",
    "outputId": "1e1746e8-2dbc-4769-b718-a8bf06add4a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is also a corpus of instant messaging chat sessions, originally collected by the Naval Postgraduate School for research on automatic detection of Internet predators.\n",
      "The corpus contains over 10,000 posts, anonymized by replacing usernames with generic names of the form 'UserNNN', and manually edited to remove any other identifying information.\n",
      "The corpus is organized into 15 files, where each file contains several hundred posts collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a generic adults chatroom).\n"
     ]
    }
   ],
   "source": [
    "print(raw_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UVXtucw-ZnA"
   },
   "source": [
    "#Segementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkMGQJqt-fhg"
   },
   "source": [
    "## Segementation by Space\n",
    "\n",
    "This method we just use space to tokenize a sentence and remove all the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JjXsMu8d-bpE",
    "outputId": "af936ca6-db9d-483a-ebfb-7d52926093c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'also', 'a', 'corpus', 'of', 'instant', 'messaging', 'chat', 'sessions']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "pun = string.punctuation\n",
    "def tokenize(sent):\n",
    "  return [x.strip() for x in re.split('(\\W+)', sent.lower()) if x.strip() and x not in pun]\n",
    "txt_tok1 = tokenize(raw_txt)\n",
    "print(txt_tok1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWEWLea4_uJA"
   },
   "source": [
    "## Segementation by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "t2SNYCf8_w9V",
    "outputId": "6b6329d8-4969-4207-e3df-5068e4086b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['there', 'is', 'also', 'a', 'corpus', 'of', 'instant', 'messaging', 'chat', 'sessions']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent = sent_tokenize(raw_txt)\n",
    "txt_tok2 = []\n",
    "for s in sent:\n",
    "  txt_tok2.extend(word_tokenize(s.lower()))\n",
    "print(txt_tok2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GoYZqMcYDSK"
   },
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dib_Z9NxYFxS"
   },
   "source": [
    "## Remove Stopwords\n",
    "\n",
    "Here we can use the stopwords corpus in NLTK directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LUpn5hJdYEzr",
    "outputId": "ea46f506-c637-46f1-b96e-aa47c8027b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ReemjOYUYefZ"
   },
   "source": [
    "Then we can see some examples of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KCdMXPWYYjj2",
    "outputId": "6470171e-c2b1-4855-e3c9-3445140e59a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1BpDvtkY3Ji"
   },
   "source": [
    "Then we need to remove the stopwords from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZKM9FO08Y75O",
    "outputId": "00fccc66-226f-473b-b8fe-95e542b42a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The orignial length of our text is 98.\n",
      "The length of our text after removing stopwords is 54.\n",
      "['also', 'corpus', 'instant', 'messaging', 'chat', 'sessions', 'originally', 'collected', 'naval', 'postgraduate']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The orignial length of our text is {len(txt_tok2)}.\")\n",
    "txt_tok2_restop = []\n",
    "for w in txt_tok2:\n",
    "  if w not in stopwords and w not in pun:\n",
    "    txt_tok2_restop.append(w)\n",
    "print(f\"The length of our text after removing stopwords is {len(txt_tok2_restop)}.\")\n",
    "print(txt_tok2_restop[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iL4z1iRgZrW5"
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QuigtfHKZwxl"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHXI1izZhwAO"
   },
   "source": [
    "A word stem is a part of word. Removing morphological affixes form words,leaving only the word stem. For example: \"word\" is the stem of \"waited\", \"waiting\" and \"waits\". We can use nltk stemmer to finish this job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPnhn-YpdBXw"
   },
   "source": [
    "We first define some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJIlcNMIczgg"
   },
   "outputs": [],
   "source": [
    "words = [\"waits\", \"waited\", \"waiting\", \"wait\", \"playing\", \"played\", \"winning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_--babX5dF9m"
   },
   "source": [
    "We import the module and use the PorterStemmer to stem our words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SimTfAYHdFA7",
    "outputId": "7564c108-acac-4467-96eb-129d95cdcf2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait', 'wait', 'wait', 'wait', 'play', 'play', 'win']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stem_word = []\n",
    "for word in words:\n",
    "  stem_word.append(ps.stem(word))\n",
    "print(stem_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gwyg8RlDd4ht"
   },
   "source": [
    "We can also stem our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "hSMCfignd7Lh",
    "outputId": "a7ed7357-8620-4e4a-e179-b2ba9b48632f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also : also\n",
      "corpus : corpu\n",
      "instant : instant\n",
      "messaging : messag\n",
      "chat : chat\n",
      "sessions : session\n",
      "originally : origin\n",
      "collected : collect\n",
      "naval : naval\n",
      "postgraduate : postgradu\n"
     ]
    }
   ],
   "source": [
    "txt_tok2_restop_stem = []\n",
    "for word in txt_tok2_restop:\n",
    "  txt_tok2_restop_stem.append(ps.stem(word))\n",
    "for i in range(10):\n",
    "  print(f\"{txt_tok2_restop[i]} : {txt_tok2_restop_stem[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5sO5ibpht6r"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNzL6Hlve7lv"
   },
   "source": [
    "## Lemmazation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paP4BbTJe9xN"
   },
   "source": [
    "Lemmazation is the process of grouping together the differrent inflected forms of a word so they can ba analysed as a single item. It's similar to stemming, but it links with similar meaning to one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8P5bsI_Xfd6i",
    "outputId": "b3e49ea4-7a49-4893-d2fb-62a8fb79ccfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "words = [\"better\", \"best\", \"good\", \"play\", \"playing\", \"cats\", \"dogs\", \"are\"]\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "L1rk91t9fy2O",
    "outputId": "94f30ed5-32b8-47a3-f0b5-5f48ab4a29bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming result: ['better', 'best', 'good', 'play', 'play', 'cat', 'dog', 'are'].\n",
      "Lemmazation result: ['better', 'best', 'good', 'play', 'play', 'cat', 'dog', 'be'].\n"
     ]
    }
   ],
   "source": [
    "stem_word = []\n",
    "lem_word = []\n",
    "for w in words:\n",
    "  stem_word.append(ps.stem(w))\n",
    "  lem_word.append(lemma.lemmatize(w, pos=\"v\"))\n",
    "print(f\"Stemming result: {stem_word}.\")\n",
    "print(f\"Lemmazation result: {lem_word}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwgB6k3LhQXv"
   },
   "source": [
    "We can also lemmatize our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "krNuPGSQhZad",
    "outputId": "4e4818ac-0487-4bac-e290-0047d43195ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also : also\n",
      "corpus : corpus\n",
      "instant : instant\n",
      "messaging : message\n",
      "chat : chat\n",
      "sessions : sessions\n",
      "originally : originally\n",
      "collected : collect\n",
      "naval : naval\n",
      "postgraduate : postgraduate\n"
     ]
    }
   ],
   "source": [
    "txt_tok2_restop_lem = []\n",
    "for word in txt_tok2_restop:\n",
    "  txt_tok2_restop_lem.append(lemma.lemmatize(word, pos='v'))\n",
    "for i in range(10):\n",
    "  print(f\"{txt_tok2_restop[i]} : {txt_tok2_restop_lem[i]}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EnglishTextPreprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
